{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAME:PATHI MAHESH KUMAR\n",
    "\n",
    "EMAIL:maheshkumarpathi.1999@gmail.com\n",
    "\n",
    "LINK: https://github.com/mahesh1271457/GlobalAIHubMLCourse/tree/main/Homeworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Generate dataset using make_blobs function in the sklearn.datasets class.                Generate 2000 samples with 3 features (X) with one label (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=2000, centers=3, n_features=3,random_state=0)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 3)\n"
     ]
    }
   ],
   "source": [
    "X, y = make_blobs(n_samples=[2000, 0, 0], centers=None, n_features=3,random_state=0)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)Explore and analyse raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration in Machine Learning\n",
    "\n",
    "A Machine Learning project is as good as the foundation of data on which it is built. In order to perform well, machine learning data exploration models must ingest large quantities of data, and model accuracy will suffer if that data is not thoroughly explored first. Data exploration steps to follow before building a machine learning model include: \n",
    "\n",
    "Variable identification: define each variable and its role in the dataset \n",
    "Univariate analysis: for continuous variables, build box plots or histograms for each variable independently; for categorical variables, build bar charts to show the frequencies\n",
    "Bi-variable analysis - determine the interaction between variables by building visualization tools\n",
    "~Continuous and Continuous: scatter plots\n",
    "~Categorical and Categorical: stacked column chart\n",
    "~Categorical and Continuous: boxplots combined with swarmplots\n",
    "Detect and treat missing values\n",
    "Detect and treat outliers\n",
    "The ultimate goal of data exploration machine learning is to provide data insights that will inspire subsequent feature engineering and the model-building process. Feature engineering facilitates the machine learning process and increases the predictive power of machine learning algorithms by creating features from raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration is the very first step in the data analysis process.\n",
    "\n",
    "Data exploration uses both manual data analysis (often considered one of the    most tedious and time consuming tasks in data science) and automated tools   that extract data into initial reports that include data visualizations and charts. This process enables deeper data analysis as patterns and trends are identified. Data exploration helps create a more straightforward view of datasets rather than pouring over thousands of figures in unstructured data.\n",
    "\n",
    "When it comes to data exploration, the most essential steps are Variable Identification, Univariate Analysis and Bi-Variate Analysis (read more about those steps here) or a tool can be used to begin acquiring knowledge of your dataset.\n",
    "\n",
    "It sounds like a lot of number crunching right? While it is a tedious preliminary step to processing data where you begin to actually interesting insights, it is a necessary evil. Why?\n",
    "\n",
    "By skipping this first exploratory step, data scientists are not be able to immediately understand key issues in the data or be able to guide deeper analysis in the right direction. Understanding and interpreting data from large data sets can be very challenging. It is difficult to understand the data set and make conclusions without looking through the entire data set. Yes, that might actually mean spending more time exploring the sample to get a better representation of the data set.\n",
    "\n",
    "Using different data exploratory data analysis methods and visualization techniques will ensure you have a richer understanding of your data. Once data exploration has uncovered connections within the data, and then are formed into different variables, it is much easier to prepare the data into charts or visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration in GIS\n",
    "GIS (Geographic Information Systems) is a framework for gathering and analyzing data connected to geographic locations and their relation to human or natural activity on Earth. With so much of the world's data now being location-enriched, geospatial analysts are faced with a rapidly increasing volume of geospatial data..\n",
    "\n",
    "![](https://assets-global.website-files.com/5deb974b5176872b2c106aba/5e72b18f5bac284a0872bccd_data-exploration-for-gis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis (EDA)\n",
    "\n",
    "Exploratory Data Analysis (EDA), similar to data exploration, is a statistical technique to analyze data sets for their broad characteristics. Visualization tools for exploratory data analysis such as OmniSci’s Immerse platform enable interactivity with raw data sets, giving analysts increased visibility into the patterns and relationships within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration in Python\n",
    "\n",
    "Data exploration with python has the advantage in ease of learning, production readiness, integration with common tools, an abundant library, and support from a huge community. Nearly every tool kit and functionality is packaged and can be executed by simply calling the name of a method.\n",
    "\n",
    "Python data exploration is made easier with Pandas, the open source Python data analysis library that can single-handedly profile any dataframe and generate a complete HTML report on the dataset. Once Pandas is imported, it allows users to import files in a variety of formats, the most popular format being CSV. The pandas data exploration library provides:\n",
    "\n",
    "Efficient dataframe object for data manipulation with integrated indexing\n",
    "Tools for reading and writing data between disparate formats\n",
    "Integrated handling of missing data and intelligent data alignment \n",
    "Flexible pivoting and reshaping of datasets\n",
    "Time series-functionality\n",
    "Intelligent label-based slicing, fancy indexing, and subsetting of large datasets\n",
    "Columns can be inserted and deleted from data structures for size mutability\n",
    "Aggregating or transforming data with a powerful group by engine allowing split-apply-combine operations on datasets\n",
    "High performance merging and joining of datasets\n",
    "Hierarchical axis indexing\n",
    "Techniques for how to improve data exploration using Pandas are discussed at length in expansive Python community forums.\n",
    "\n",
    "‍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration in R\n",
    "The data exploration and visualization with R process looks like:\n",
    "\n",
    "Loading the data: Due to the availability of predefined libraries and simple syntax, loading data from a variety of formats, such as .XLS, TXT, CSV, and JSON, is very straightforward\n",
    "Converting variables: The process of converting a variable into a different data type in R entails adding a character string to a numeric vector, converting all the elements in the vector to the character\n",
    "Transpose a dataset: R provides code to transpose a dataset from a wide structure to a much narrower structure\n",
    "Sorting of dataframe: accomplished by using order as an index\n",
    "Create plots or histograms\n",
    "Generate frequency tables to best understand the distribution across categories\n",
    "Generate a sample set with just a few random indices\n",
    "Remove duplicate values of a variable\n",
    "Find class-level count average and sum: R data exploration techniques include apply functions to accomplish this\n",
    "Recognize and treat missing values and outliers by inputting with the mean of other numbers\n",
    "Merge and join datasets: R includes an appending datasets function and a bind function\n",
    "‍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)Do preprocessing for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a data mining technique that transforms raw data into an understandable format. Raw data(real world data) is always incomplete and that data cannot be sent through a model. That would cause certain errors. That is why we need to preprocess data before sending through a model.\n",
    "Steps in Data Preprocessing\n",
    "Here are the steps I have followed;\n",
    "1. Import libraries\n",
    "2. Read data\n",
    "3. Checking for missing values\n",
    "4. Checking for categorical data\n",
    "5. Standardize the data\n",
    "6. PCA transformation\n",
    "7. Data splitting\n",
    "1. Import Data\n",
    "As main libraries, I am using Pandas, Numpy and time;\n",
    "Pandas: Use for data manipulation and data analysis.\n",
    "Numpy: a fundamental package for scientific computing with Python.\n",
    "As for the visualization I am using Matplotlib and Seaborn.\n",
    "For the data preprocessing techniques and algorithms, I used Scikit-learn libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1f328a6a8d00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmatthews_corrcoef\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#### main libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "##### visual libraries\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "plt.style.use('ggplot')\n",
    "##### sklearn libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score,matthews_corrcoef,classification_report,roc_curve\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv('C:\\Users\\pathi mahesh kumar\\Downloads\\features.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)Split your dataset into train and test test (0.7 for train and 0.3 for test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.83059382  6.70631054]\n",
      " [ 9.15602113 -3.85645821]\n",
      " [ 8.50970095 -4.05073206]\n",
      " [ 4.06001955  5.2775025 ]\n",
      " [ 3.84874278  4.93254193]]\n",
      "[[ 5.83059382  6.70631054]\n",
      " [ 9.15602113 -3.85645821]\n",
      " [ 8.50970095 -4.05073206]\n",
      " [ 4.06001955  5.2775025 ]\n",
      " [ 3.84874278  4.93254193]]\n"
     ]
    }
   ],
   "source": [
    "# split again, and we should see the same split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=1)\n",
    "# demonstrate that the train-test split procedure is repeatable\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "# create dataset\n",
    "X, y = make_blobs(n_samples=100)\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "# summarize first 5 rows\n",
    "print(X_train[:5, :])\n",
    "# split again, and we should see the same split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "# summarize first 5 rows\n",
    "print(X_train[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)Try Decision Tree and XGBoost Algorithm with different hyperparameters. (Using GridSearchCV is a plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1050/1*Xfnh-biDrMCECEO37qecKQ.png)\n",
    "\n",
    "Outline:\n",
    "\n",
    "1.Results\n",
    "\n",
    "2.Hyperparameter tuning overview\n",
    "\n",
    "3.Bayesian optimization\n",
    "\n",
    "4.Early stopping\n",
    "\n",
    "5.Implementation details\n",
    "\n",
    "6.Baseline linear regression\n",
    "\n",
    "7.ElasticNetCV (Linear regression with L1 and L2 regularization)\n",
    "\n",
    "8.ElasticNet with GridSearchCV\n",
    "\n",
    "9.XGBoost: sequential grid search over hyperparameter subsets with early \n",
    "\n",
    "10.stopping\n",
    "\n",
    "11.XGBoost: Hyperopt and Optuna search algorithms\n",
    "\n",
    "12.LightGBM: Hyperopt and Optuna search algorithms\n",
    "\n",
    "13.XGBoost on a Ray cluster\n",
    "\n",
    "14.LightGBM on a Ray cluster\n",
    "\n",
    "Concluding remarks\n",
    "\n",
    "1. Results\n",
    "\n",
    "Bottom line up front: Here are results on the Ames housing data set, predicting Iowa home prices:\n",
    "\n",
    "Baseline linear models\n",
    "Times for single-instance are on a local desktop with 12 threads, comparable to EC2 4xlarge. Times for cluster are on m5.large x 32 (1 head node + 31 workers).\n",
    "We obtain a big speedup when using Hyperopt and Optuna locally, compared to grid search. The sequential search performed about 261 trials, so the XGB/Optuna search performed about 3x as many trials in half the time and got a similar result.\n",
    "The cluster of 32 instances (64 threads) gave a modest RMSE improvement vs. the local desktop with 12 threads. I tried to set this up so we would get some improvement in RMSE vs. local Hyperopt/Optuna (which we did with 2048 trials), and some speedup in training time (which we did not get with 64 threads). It ran twice the number of trials in slightly less than twice the time. The comparison is imperfect, local desktop vs. AWS, running Ray 1.0 on local and 1.1 on the cluster, different number of trials (better hyperparameter configs don’t get early-stopped and take longer to train). But the point was to see what kind of improvement one might obtain in practice, leveraging a cluster vs. a local desktop or laptop. Bottom line, modest benefit here from a 32-node cluster.\n",
    "RMSEs are similar across the board. XGB with 2048 trials is best by a small margin among the boosting models.\n",
    "LightGBM doesn’t offer an improvement over XGBoost here in RMSE or run time. In my experience, LightGBM is often faster, so you can train and tune more in a given time. But we don’t see that here. Possibly XGB interacts better with ASHA early stopping.\n",
    "Similar RMSE between Hyperopt and Optuna. Optuna is consistently faster (up to 35% with LGBM/cluster).\n",
    "Our simple ElasticNet baseline yields slightly better results than boosting, in seconds. This may be because our feature engineering was intensive and designed to fit the linear model. Not shown, SVR and KernelRidge outperform ElasticNet, and an ensemble improves over all individual algos.\n",
    "Full notebooks are on GitHub.\n",
    "\n",
    "2. Hyperparameter Tuning Overview\n",
    "\n",
    "(If you are not a data scientist ninja, here is some context. If you are, you can safely skip to the Bayesian Optimization section and the implementations below.)\n",
    "Any sufficiently advanced machine learning model is indistinguishable from magic, and any sufficiently advanced machine learning model needs good tuning.\n",
    "Backing up a step, here is a typical modeling workflow:\n",
    "Exploratory data analysis: understand your data.\n",
    "Feature engineering and feature selection: clean, transform and engineer the best possible features\n",
    "Modeling: model selection and hyperparameter tuning to identify the best model architecture, and ensembling to combine multiple models\n",
    "Evaluation: Describe the out-of-sample error and its expected distribution.\n",
    "To minimize the out-of-sample error, you minimize the error from bias, meaning the model isn’t sufficiently sensitive to the signal in the data, and variance, meaning the model is too sensitive to the signal specific to the training data in ways that don’t generalize out-of-sample. Modeling is 90% data prep, the other half is all finding the optimal bias-variance tradeoff.\n",
    "Hyperparameters help you tune the bias-variance tradeoff. For a simple logistic regression predicting survival on the Titanic, a regularization parameter lets you control overfitting by penalizing sensitivity to any individual feature. For a massive neural network doing machine translation, the number and types of layers, units, activation function, in addition to regularization, are hyperparameters. We select the best hyperparameters using k-fold cross-validation; this is what we call hyperparameter tuning.\n",
    "The regression algorithms we use in this post are XGBoost and LightGBM, which are variations on gradient boosting. Gradient boosting is an ensembling method that usually involves decision trees. A decision tree constructs rules like, if the passenger is in first class and female, they probably survived the sinking of the Titanic. Trees are powerful, but a single deep decision tree with all your features will tend to overfit the training data. A random forest algorithm builds many decision trees based on random subsets of observations and features which then vote (bagging). The outcome of a vote by weak learners is less overfitted than training on all the data rows and all the feature columns to generate a single strong learner and performs better out-of-sample. Random forest hyperparameters include the number of trees, tree depth, and how many features and observations each tree should use.\n",
    "Instead of aggregating many independent learners working in parallel, i.e. bagging, boosting uses many learners in series:\n",
    "Start with a simple estimate like the median or base rate.\n",
    "Fit a tree to the error in this prediction.\n",
    "If you can predict the error, you can adjust for it and improve the prediction. Adjust the prediction not all the way to the tree prediction, but part of the way based on a learning rate (a hyperparameter).\n",
    "Fit another tree to the error in the updated prediction and adjust the prediction further based on the learning rate.\n",
    "Iteratively continue reducing the error for a specified number of boosting rounds (another hyperparameter).\n",
    "The final estimate is the initial prediction plus the sum of all the predicted necessary adjustments (weighted by the learning rate).\n",
    "The learning rate performs a similar function to voting in random forest, in the sense that no single decision tree determines too much of the final estimate. This ‘wisdom of crowds’ approach helps prevent overfitting.\n",
    "Gradient boosting is the current state of the art for regression and classification on traditional structured tabular data (in contrast to less structured data like image/video/natural language processing, where deep learning, i.e. deep neural nets are state of the art).\n",
    "Gradient boosting algorithms like XGBoost, LightGBM, and CatBoost have a very large number of hyperparameters, and tuning is an important part of using them.\n",
    "These are the principal approaches to hyperparameter tuning:\n",
    "Grid search: Given a finite set of discrete values for each hyperparameter, exhaustively cross-validate all combinations.\n",
    "Random search: Given a discrete or continuous distribution for each hyperparameter, randomly sample from the joint distribution. Generally more efficient than exhaustive grid search.\n",
    "Bayesian optimization: Sample like random search, but update the search space you sample from as you go, based on outcomes of prior searches.\n",
    "Gradient-based optimization: Attempt to estimate the gradient of the cross-validation metric with respect to the hyperparameters and ascend/descend the gradient.\n",
    "Evolutionary optimization: Sample the search space, discard combinations with poor metrics, and genetically evolve new combinations based on the successful combinations.\n",
    "Population-based training: A method of performing hyperparameter optimization at the same time as training.\n",
    "In this post, we focus on Bayesian optimization with Hyperopt and Optuna.\n",
    "\n",
    "3. Bayesian Optimization\n",
    "\n",
    "What is Bayesian optimization? When we perform a grid search, the search space is a prior: we believe that the best hyperparameter vector is in this search space. And a priori perhaps each hyperparameter combination has equal probability of being the best combination (a uniform distribution). So we try them all and pick the best one.\n",
    "Perhaps we might do two passes of grid search. After an initial search on a broad, coarsely spaced grid, we do a deeper dive in a smaller area around the best metric from the first pass, with a more finely-spaced grid. In Bayesian terminology, we updated our prior.\n",
    "Bayesian optimization starts by sampling randomly, e.g. 30 combinations, and computes the cross-validation metric for each of the 30 randomly sampled combinations using k-fold cross-validation. Then the algorithm updates the distribution it samples from, so that it is more likely to sample combinations similar to the good metrics, and less likely to sample combinations similar to the poor metrics. As it continues to sample, it continues to update the search distribution it samples from, based on the metrics it finds.\n",
    "Image for post\n",
    "Image by Author\n",
    "Good metrics are generally not uniformly distributed. If they are found close to one another in a Gaussian distribution or any distribution which we can model, then Bayesian optimization can exploit the underlying pattern, and is likely to be more efficient than grid search or naive random search.\n",
    "HyperOpt is a Bayesian optimization algorithm by James Bergstra et al., see this excellent blog post by Subir Mansukhani.\n",
    "Optuna is a Bayesian optimization algorithm by Takuya Akiba et al., \n",
    "\n",
    "see this \n",
    "excellent blog post by Crissman Loomis.\n",
    "\n",
    "4. Early Stopping\n",
    "\n",
    "If, while evaluating a hyperparameter combination, the evaluation metric is not improving in training, or not improving fast enough to beat our best to date, we can discard a combination before fully training on it. Early stopping of unsuccessful training runs increases the speed and effectiveness of our search.\n",
    "XGBoost and LightGBM helpfully provide early stopping callbacks to check on training progress and stop a training trial early (XGBoost; LightGBM). Hyperopt, Optuna, and Ray use these callbacks to stop bad trials quickly and accelerate performance.\n",
    "In this post, we will use the Asynchronous Successive Halving Algorithm (ASHA) for early stopping, described in this blog post.\n",
    "Further reading:\n",
    "Hyper-Parameter Optimization: A Review of Algorithms and Applications Tong Yu, Hong Zhu (2020)\n",
    "Hyperparameter Search in Machine Learning, Marc Claesen, Bart De Moor (2015)\n",
    "Hyperparameter Optimization, Matthias Feurer, Frank Hutter (2019)\n",
    "\n",
    "5. Implementation Details\n",
    "\n",
    "We use data from the Ames Housing Dataset. The original data set has 79 raw features. The data we will use has 100 features with a fair amount of feature engineering from my own attempt at modeling, which was in the top 5% or so when I submitted it to Kaggle. We model the log of the sale price, and use RMSE as our metric for model selection. We convert the RMSE back to raw dollar units for easier interpretability.\n",
    "We use 4 regression algorithms:\n",
    "LinearRegression: baseline with no hyperparameters\n",
    "ElasticNet: Linear regression with L1 and L2 regularization (2 hyperparameters).\n",
    "XGBoost\n",
    "LightGBM\n",
    "\n",
    "We use 5 approaches:\n",
    "\n",
    "Native CV: In sklearn if an algorithm xxx has hyperparameters it will often have an xxxCV version, like ElasticNetCV, which performs automated grid search over hyperparameter iterators with specified kfolds.\n",
    "GridSearchCV: Abstract grid search that can wrap around any sklearn algorithm, running multithreaded trials over specified kfolds.\n",
    "Manual sequential grid search: How we typically implement grid search with XGBoost, which doesn’t play very well with GridSearchCV and has too many hyperparameters to tune in one pass.\n",
    "Ray Tune on local desktop: Hyperopt and Optuna with ASHA early stopping.\n",
    "Ray Tune on AWS cluster: Additionally scale out to run a single hyperparameter optimization task over many instances in a cluster.\n",
    "\n",
    "6. Baseline linear regression\n",
    "\n",
    "Use the same kfolds for each run so the variation in the RMSE metric is not due to variation in kfolds.\n",
    "We fit on the log response, so we convert error back to dollar units, for interpretability.\n",
    "sklearn.model_selection.cross_val_score for evaluation\n",
    "Jupyter %%time magic for wall time\n",
    "n_jobs=-1 to run folds in parallel using all CPU cores available.\n",
    "Note the wall time < 1 second and RMSE of 18192.\n",
    "Full notebooks are on GitHub.\n",
    "%%time\n",
    "\n",
    "#### always use same RANDOM_STATE k-folds for comparability between tests, reproducibility\n",
    "RANDOMSTATE = 42\n",
    "np.random.seed(RANDOMSTATE)\n",
    "\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=RANDOMSTATE)\n",
    "\n",
    "MEAN_RESPONSE=df[response].mean()\n",
    "def cv_to_raw(cv_val, mean_response=MEAN_RESPONSE):\n",
    "    \"\"\"convert log1p rmse to underlying SalePrice error\"\"\"\n",
    "    # MEAN_RESPONSE assumes folds have same mean response, which is true in expectation but not in each fold\n",
    "    # we can also pass the mean response for each fold\n",
    "    # but we're really just looking to consistently convert the log value to a more meaningful unit\n",
    "    return np.expm1(mean_response+cv_val) - np.expm1(mean_response)\n",
    "    \n",
    "lr = LinearRegression()\n",
    "### compute CV metric for each fold\n",
    "scores = -cross_val_score(lr, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "Raw CV RMSE 18192 (STD 1839)\n",
    "Wall time: 65.4 ms\n",
    "\n",
    "7. ElasticNetCV\n",
    "\n",
    "ElasticNet is linear regression with L1 and L2 regularization (2 hyperparameters).\n",
    "When we use regularization, we need to scale our data so that the coefficient penalty has a similar impact across features. We use a pipeline with RobustScaler for scaling.\n",
    "Fit a model and extract hyperparameters from the fitted model.\n",
    "Then we do cross_val_score with reported hyperparams (There doesn't appear to be a way to extract the score from the fitted model without refitting)\n",
    "Verbose output reports 130 tasks, for full grid search on 10 folds we would expect 13x9x10=1170. Apparently a clever optimization.\n",
    "Note the modest reduction in RMSE vs. linear regression without regularization.\n",
    "elasticnetcv = make_pipeline(RobustScaler(),\n",
    "                             ElasticNetCV(max_iter=100000, \n",
    "                                          l1_ratio=[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99],\n",
    "                                          alphas=np.logspace(-4, -2, 9),\n",
    "                                          cv=kfolds,\n",
    "                                          n_jobs=-1,\n",
    "                                          verbose=1,\n",
    "                                         ))\n",
    "\n",
    "#train and get hyperparams\n",
    "elasticnetcv.fit(df[predictors], df[response])\n",
    "l1_ratio = elasticnetcv._final_estimator.l1_ratio_\n",
    "alpha = elasticnetcv._final_estimator.alpha_\n",
    "print('l1_ratio', l1_ratio)\n",
    "print('alpha', alpha)\n",
    "\n",
    "#### evaluate using kfolds on full dataset\n",
    "\n",
    "elasticnet = ElasticNet(alpha=alpha,\n",
    "                        l1_ratio=l1_ratio,\n",
    "                        max_iter=10000)\n",
    "\n",
    "scores = -cross_val_score(elasticnet, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.04f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "l1_ratio 0.01\n",
    "alpha 0.0031622776601683794\n",
    "\n",
    "Log1p CV RMSE 0.1030 (STD 0.0109)\n",
    "Raw CV RMSE 18061 (STD 2008)\n",
    "CPU times: user 5.93 s, sys: 3.67 s, total: 9.6 s\n",
    "Wall time: 1.61 s\n",
    "\n",
    "\n",
    "8. GridSearchCV\n",
    "\n",
    "\n",
    "Identical result, runs a little slower.\n",
    "GridSearchCV verbose output shows 1170 jobs, which is the expected number 13x9x10.\n",
    "gs = make_pipeline(RobustScaler(),\n",
    "                   GridSearchCV(ElasticNet(max_iter=100000),\n",
    "                                param_grid={'l1_ratio': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99],\n",
    "                                            'alpha': np.logspace(-4, -2, 9),\n",
    "                                           },\n",
    "                                scoring='neg_root_mean_squared_error',\n",
    "                                refit=True,\n",
    "                                cv=kfolds,\n",
    "                                n_jobs=-1,\n",
    "                                verbose=1\n",
    "                               ))\n",
    "\n",
    "Wall time: 5 s\n",
    "\n",
    "9. XGBoost with sequential grid search\n",
    "\n",
    "It should be possible to use GridSearchCV with XGBoost. But when we also try to use early stopping, XGBoost wants an eval set. OK, we can give it a static eval set held out from GridSearchCV. Now, GridSearchCV does k-fold cross-validation in the training set but XGBoost uses a separate dedicated eval set for early stopping. It’s a bit of a Frankenstein methodology. See the notebook for the attempt at GridSearchCV with XGBoost and early stopping if you’re really interested.\n",
    "Instead, we write our own grid search that gives XGBoost the correct hold-out set for each CV fold:\n",
    "EARLY_STOPPING_ROUNDS=100  # stop if no improvement after 100 rounds\n",
    "\n",
    "def my_cv(df, predictors, response, kfolds, regressor, verbose=False):\n",
    "    \"\"\"Roll our own CV \n",
    "    train each kfold with early stopping\n",
    "    return average metric, sd over kfolds, average best round\"\"\"\n",
    "    metrics = []\n",
    "    best_iterations = []\n",
    "\n",
    "    for train_fold, cv_fold in kfolds.split(df): \n",
    "        fold_X_train=df[predictors].values[train_fold]\n",
    "        fold_y_train=df[response].values[train_fold]\n",
    "        fold_X_test=df[predictors].values[cv_fold]\n",
    "        fold_y_test=df[response].values[cv_fold]\n",
    "        regressor.fit(fold_X_train, fold_y_train,\n",
    "                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                      eval_set=[(fold_X_test, fold_y_test)],\n",
    "                      eval_metric='rmse',\n",
    "                      verbose=verbose\n",
    "                     )\n",
    "        y_pred_test=regressor.predict(fold_X_test)\n",
    "        metrics.append(np.sqrt(mean_squared_error(fold_y_test, y_pred_test)))\n",
    "        best_iterations.append(regressor.best_iteration)\n",
    "    return np.average(metrics), np.std(metrics), np.average(best_iterations)\n",
    "    \n",
    "XGBoost has many tuning parameters so an exhaustive grid search has an unreasonable number of combinations. Instead, we tune reduced sets sequentially using grid search and use early stopping.\n",
    "This is the typical grid search methodology to tune XGBoost:\n",
    "\n",
    "XGBoost tuning methodology\n",
    "\n",
    "Set an initial set of starting parameters.\n",
    "\n",
    "Tune sequentially on groups of hyperparameters that don’t interact too much between groups, to reduce the number of combinations tested.\n",
    "First, tune max_depth.\n",
    "Then tune subsample, colsample_bytree, and colsample_bylevel.\n",
    "Finally, tune learning rate: a lower learning rate will need more boosting rounds (n_estimators).\n",
    "\n",
    "Do 10-fold cross-validation on each hyperparameter combination.\n",
    "\n",
    "Pick hyperparameters to minimize average RMSE over kfolds.\n",
    "Use XGboost early stopping to halt training in each fold if no improvement after 100 rounds.\n",
    "After tuning and selecting the best hyperparameters, retrain and evaluate on the full dataset without early stopping, using the average boosting rounds across xval kfolds.\n",
    "\n",
    "As discussed, we use the XGBoost sklearn API and roll our own grid search which understands early stopping with k-folds, instead of GridSearchCV. (An alternative would be to use native xgboost .cv which understands early stopping but doesn’t use sklearn API (uses DMatrix, not numpy array or dataframe))\n",
    "We write a helper function cv_over_param_dict which takes a list of param_dict dictionaries, runs trials over all dictionaries, and returns the best param_dict dictionary plus a dataframe of results.\n",
    "We run cv_over_param_dict 3 times to do 3 grid searches over our 3 tuning rounds.\n",
    "BOOST_ROUNDS=50000   # we use early stopping so make this arbitrarily high\n",
    "\n",
    "def cv_over_param_dict(df, param_dict, predictors, response, kfolds, verbose=False):\n",
    "    \"\"\"given a list of dictionaries of xgb params\n",
    "    run my_cv on params, store result in array\n",
    "    return updated param_dict, results dataframe\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, d in enumerate(param_dict):\n",
    "        xgb = XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=BOOST_ROUNDS,\n",
    "            random_state=RANDOMSTATE,    \n",
    "            verbosity=1,\n",
    "            n_jobs=-1,\n",
    "            booster='gbtree',   \n",
    "            **d\n",
    "        )    \n",
    "\n",
    "        metric_rmse, metric_std, best_iteration = my_cv(df, predictors, response, kfolds, xgb, verbose=False)    \n",
    "        results.append([metric_rmse, metric_std, best_iteration, d])\n",
    "    \n",
    "        print(\"%s %3d result mean: %.6f std: %.6f, iter: %.2f\" % (datetime.strftime(datetime.now(), \"%T\"), i, metric_rmse, metric_std, best_iteration))\n",
    "        \n",
    "    end_time = datetime.now()\n",
    "    print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "    print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "    print(str(timedelta(seconds=(end_time-start_time).seconds)))\n",
    "    \n",
    "    results_df = pd.DataFrame(results, columns=['rmse', 'std', 'best_iter', 'param_dict']).sort_values('rmse')\n",
    "    display(results_df.head())\n",
    "    \n",
    "    best_params = results_df.iloc[0]['param_dict']\n",
    "    return best_params, results_df\n",
    "\n",
    "#### initial hyperparams\n",
    "current_params = {\n",
    "    'max_depth': 5,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'colsample_bylevel': 0.5,\n",
    "    'subsample': 0.5,\n",
    "    'learning_rate': 0.01,\n",
    "}\n",
    "\n",
    "#### cv and get best params\n",
    "\n",
    "current_params, results_df = cv_over_param_dict(df, full_search_dicts, predictors, response, kfolds)\n",
    "\n",
    "#### round 3: learning rate\n",
    "\n",
    "learning_rates = np.logspace(-3, -1, 5)\n",
    "grid_search_dicts = [{'learning_rate': lr} for lr in learning_rates]\n",
    "\n",
    "\n",
    "#### cv and get best params\n",
    "\n",
    "current_params, results_df = cv_over_param_dict(df, full_search_dicts, predictors, response, kfolds, verbose=False)\n",
    "The total training duration (the sum of times over the 3 iterations) is 1:24:22. This time may be an underestimate, since this search space is based on prior experience.\n",
    "Finally, we refit using the best hyperparameters and evaluate:\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=3438,\n",
    "    random_state=RANDOMSTATE,    \n",
    "    verbosity=1,\n",
    "    n_jobs=-1,\n",
    "    booster='gbtree',   \n",
    "    **current_params\n",
    ")    \n",
    "\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "The result essentially matches linear regression but is not as good as ElasticNet.\n",
    "Raw CV RMSE 18193 (STD 2461)\n",
    "\n",
    "10. XGBoost with Hyperopt, Optuna, and Ray\n",
    "\n",
    "The steps to run a Ray tuning job with Hyperopt are:\n",
    "Set up a Ray search space as a config dict.\n",
    "Refactor the training loop into a function which takes the config dict as an argument and calls tune.report(rmse=rmse) to optimize a metric like RMSE.\n",
    "Call ray.tune with the config and a num_samples argument which specifies how many times to sample.\n",
    "Set up the Ray search space:\n",
    "xgb_tune_kwargs = {\n",
    "    \"n_estimators\": tune.loguniform(100, 10000),\n",
    "    \"max_depth\": tune.randint(0, 5),\n",
    "    \"subsample\": tune.quniform(0.25, 0.75, 0.01),\n",
    "    \"colsample_bytree\": tune.quniform(0.05, 0.5, 0.01),\n",
    "    \"colsample_bylevel\": tune.quniform(0.05, 0.5, 0.01),    \n",
    "    \"learning_rate\": tune.quniform(-3.0, -1.0, 0.5),  # powers of 10\n",
    "}\n",
    "\n",
    "xgb_tune_params = [k for k in xgb_tune_kwargs.keys() if k != 'wandb']\n",
    "xgb_tune_params\n",
    "Set up the training function. Note that some search algos expect all hyperparameters to be floats and some search intervals to start at 0. So we convert params as necessary.\n",
    "def my_xgb(config):\n",
    "\n",
    "    # fix these configs to match calling convention\n",
    "    # search wants to pass in floats but xgb wants ints\n",
    "    config['n_estimators'] = int(config['n_estimators'])   # pass float eg loguniform distribution, use int\n",
    "    # hyperopt needs left to start at 0 but we want to start at 2    \n",
    "    config['max_depth'] = int(config['max_depth']) + 2\n",
    "    config['learning_rate'] = 10 ** config['learning_rate']\n",
    "    \n",
    "    xgb = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_jobs=1,\n",
    "        random_state=RANDOMSTATE,\n",
    "        booster='gbtree',   \n",
    "        scale_pos_weight=1, \n",
    "        **config,\n",
    "    )\n",
    "    scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                                      scoring=\"neg_root_mean_squared_error\",\n",
    "                                      cv=kfolds)\n",
    "    rmse = np.mean(scores)\n",
    "    tune.report(rmse=rmse)\n",
    "    \n",
    "    return {\"rmse\": rmse}\n",
    "Run Ray Tune:\n",
    "algo = HyperOptSearch(random_state_seed=RANDOMSTATE)\n",
    "#### ASHA\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "analysis = tune.run(my_xgb,\n",
    "                    num_samples=NUM_SAMPLES,\n",
    "                    config=xgb_tune_kwargs,                    \n",
    "                    name=\"hyperopt_xgb\",\n",
    "                    metric=\"rmse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "                   )\n",
    "Extract the best hyperparameters, and evaluate a model using them:\n",
    "#### results dataframe sorted by best metric\n",
    "param_cols = ['config.' + k for k in xgb_tune_params]\n",
    "analysis_results_df = analysis.results_df[['rmse', 'date', 'time_this_iter_s'] + param_cols].sort_values('rmse')\n",
    "\n",
    "#### extract top row\n",
    "best_config = {z: analysis_results_df.iloc[0]['config.' + z] for z in xgb_tune_params}\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=RANDOMSTATE,    \n",
    "    verbosity=1,\n",
    "    n_jobs=-1,\n",
    "    **best_config\n",
    ")\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "With NUM_SAMPLES=1024 we obtain:\n",
    "Raw CV RMSE 18309 (STD 2428)\n",
    "We can swap out Hyperopt for Optuna as simply as:\n",
    "algo = OptunaSearch()\n",
    "With NUM_SAMPLES=1024 we obtain:\n",
    "Raw CV RMSE 18325 (STD 2473)\n",
    "\n",
    "11. LightGBM with Hyperopt and Optuna\n",
    "\n",
    "We can also easily swap out XGBoost for LightGBM.\n",
    "Update the search space using LightGBM equivalents.\n",
    "lgbm_tune_kwargs = {\n",
    "       \"n_estimators\": tune.loguniform(100, 10000),\n",
    "       \"max_depth\": tune.randint(0, 5),\n",
    "       'num_leaves': tune.quniform(1, 10, 1.0),               # xgb max_leaves\n",
    "       \"bagging_fraction\": tune.quniform(0.5, 0.8, 0.01),    # xgb subsample\n",
    "       \"feature_fraction\": tune.quniform(0.05, 0.5, 0.01),   # xgb colsample_bytree\n",
    "       \"learning_rate\": tune.quniform(-3.0, -1.0, 0.5),\n",
    "   }\n",
    "Update training function:\n",
    "def my_lgbm(config):\n",
    "    \n",
    "    # fix these configs \n",
    "    config['n_estimators'] = int(config['n_estimators'])   # pass float eg loguniform distribution, use int\n",
    "    config['num_leaves'] = int(2**config['num_leaves'])\n",
    "    config['learning_rate'] = 10**config['learning_rate']\n",
    "    \n",
    "    lgbm = LGBMRegressor(objective='regression',\n",
    "                         max_bin=200,\n",
    "                         feature_fraction_seed=7,\n",
    "                         min_data_in_leaf=2,\n",
    "                         verbose=-1,\n",
    "                         n_jobs=1,\n",
    "                         # these are specified to suppress warnings\n",
    "                         colsample_bytree=None,\n",
    "                         min_child_samples=None,\n",
    "                         subsample=None,\n",
    "                         **config,\n",
    "                         )\n",
    "    \n",
    "    scores = -cross_val_score(lgbm, df[predictors], df[response],\n",
    "                              scoring=\"neg_root_mean_squared_error\",\n",
    "                              cv=kfolds)\n",
    "    rmse=np.mean(scores)  \n",
    "    tune.report(rmse=rmse)\n",
    "    \n",
    "    return {'rmse': np.mean(scores)}\n",
    "    \n",
    "and run as before, swapping my_lgbm in place of my_xgb. Results for LGBM: (NUM_SAMPLES=1024):\n",
    "Raw CV RMSE 18615 (STD 2356)\n",
    "Swapping out Hyperopt for Optuna:\n",
    "Raw CV RMSE 18614 (STD 2423)\n",
    "\n",
    "12. XGBoost on a Ray cluster\n",
    "\n",
    "Ray is a distributed framework. We can run a Ray Tune job over many instances using a cluster with a head node and many worker nodes.\n",
    "Launching Ray is straightforward. On the head node we run ray start. On each worker node we run ray start --address x.x.x.x with the address of the head node. Then in python we call ray.init() to connect to the head node. Everything else proceeds as before, and the head node runs trials using all instances in the cluster and stores results in Redis.\n",
    "Where it gets more complicated is specifying all the AWS details, instance types, regions, subnets, etc.\n",
    "Clusters are defined in ray1.1.yaml. (So far in this notebook we have been using the current production ray 1.0, but I had difficulty getting a cluster to run with ray 1.0 so I switched to the dev nightly. YMMV.)\n",
    "boto3 and AWS CLI configured credentials are used to spawn instances, so install and configure AWS CLI\n",
    "Edit ray1.1.yaml file with, at a minimum, your AWS region and availability zone. Imageid may vary across regions, search for the current Deep Learning AMI (Ubuntu 18.04). You may not need to specify subnet, I had an issue with an inaccessible subnet when I let Ray default the subnet, possibly bad defaults somewhere.\n",
    "\n",
    "To obtain those variables, launch the latest Deep Learning AMI (Ubuntu 18.04) currently Version 35.0 into a small instance in your favorite region/zone\n",
    "Test that it works\n",
    "Note the 4 variables: region, availability zone, subnet, AMI imageid\n",
    "Terminate the instance and edit ray1.1.yaml with your region, availability zone, AMI imageid, optionally subnet\n",
    "It may be advisable create your own image with all updates and requirements pre-installed and specify its AMI imageid, instead of using the generic image and installing everything at launch.\n",
    "To run the cluster: ray up ray1.1.yaml\n",
    "Creates head instance using AMI specified.\n",
    "\n",
    "Installs Ray and related requirements including XGBoost from requirements.txt\n",
    "Clones the druce/iowa repo from GitHub\n",
    "Launches worker nodes per auto-scaling parameters (currently we fix the number of nodes because we’re not benchmarking the time the cluster will take to auto-scale)\n",
    "After the cluster starts you can check the AWS console and note that several instances were launched.\n",
    "Check ray monitor ray1.1.yaml for any error messages\n",
    "Run Jupyter on the cluster with port forwarding ray exec ray1.1.yaml --port-forward=8899 'jupyter notebook --port=8899'\n",
    "Open the notebook on the generated URL which is printed on the console at startup e.g. http://localhost:8899/?token=5f46d4355ae7174524ba71f30ef3f0633a20b19a204b93b4\n",
    "You can run a terminal on the head node of the cluster with ray attach /Users/drucev/projects/iowa/ray1.1.yaml\n",
    "You can ssh explicitly with the IP address and the generated private key ssh -o IdentitiesOnly=yes -i ~/.ssh/ray-autoscaler_1_us-east-1.pem ubuntu@54.161.200.54\n",
    "Run port forwarding to the Ray dashboard with\n",
    "ray dashboard ray1.1.yaml and then open http://localhost:8265/\n",
    "Make sure to choose the default kernel in Jupyter to run in the correct conda environment with all installs\n",
    "Make sure to use the ray.init() command given in the startup messages. ray.init(address='localhost:6379', _redis_password='5241590000000000')\n",
    "The cluster will incur AWS charges so ray down ray1.1.yaml when complete\n",
    "See hyperparameter_optimization_cluster.ipynb, separated out so each notebook can be run end-to-end with/without cluster setup\n",
    "See Ray docs for additional info on Ray clusters.\n",
    "\n",
    "Besides connecting to the cluster instead of running Ray Tune locally, no other change to code is needed to run on the cluster\n",
    "analysis = tune.run(my_xgb,\n",
    "                    num_samples=NUM_SAMPLES,\n",
    "                    config=xgb_tune_kwargs,                    \n",
    "                    name=\"hyperopt_xgb\",\n",
    "                    metric=\"rmse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    # add this because distributed jobs occasionally error out\n",
    "                    raise_on_failed_trial=False, # otherwise no reults df returned if any trial error           \n",
    "                    verbose=1,\n",
    "                   )\n",
    "Results for XGBM on cluster (2048 samples, cluster is 32 m5.large instances):\n",
    "Hyperopt (time 1:30:58)\n",
    "Raw CV RMSE 18030 (STD 2356)\n",
    "Optuna (time 1:29:57)\n",
    "Raw CV RMSE 18028 (STD 2353)\n",
    "\n",
    "13. LightGBM on a Ray cluster\n",
    "\n",
    "Similarly for LightGBM:\n",
    "analysis = tune.run(my_lgbm,\n",
    "                    num_samples=NUM_SAMPLES,\n",
    "                    config = lgbm_tune_kwargs,\n",
    "                    name=\"hyperopt_lgbm\",\n",
    "                    metric=\"rmse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    raise_on_failed_trial=False, # otherwise no reults df returned if any trial error                                                            \n",
    "                    verbose=1,\n",
    "                   )\n",
    "                   \n",
    "Results for LightGBM on cluster (2048 samples, cluster is 32 m5.large instances):\n",
    "Hyperopt (time: 1:05:19) :\n",
    "Raw CV RMSE 18459 (STD 2511)\n",
    "Optuna (time 0:48:16):\n",
    "Raw CV RMSE 18458 (STD 2511)\n",
    "\n",
    "14. Concluding remarks\n",
    "\n",
    "In every case I’ve applied them, Hyperopt and Optuna have given me at least a small improvement in the best metrics I found using grid search methods. Bayesian optimization tunes faster with a less manual process vs. sequential tuning. It’s fire-and-forget.\n",
    "Is Ray Tune the way to go for hyperparameter tuning? Provisionally, yes. Ray provides integration between the underlying ML (e.g. XGBoost), the Bayesian search (e.g. Hyperopt), and early stopping (ASHA). It allows us to easily swap search algorithms.\n",
    "\n",
    "There are other alternative search algorithms in the Ray docs but these seem to be the most popular, and I haven’t got the others to run yet. If after a while I find I am always using e.g. Hyperopt and never use clusters, I might use the native Hyperopt/XGBoost integration without Ray, to access any native Hyperopt features and because it’s one less technology in the stack.\n",
    "\n",
    "Clusters? Most of the time I don’t have a need, costs add up, did not see as large a speedup as expected. I only see ~2x speedup on the 32-instance cluster. Setting up the test I expected a bit less than 4x speedup accounting for slightly less-than-linear scaling. The longest run I have tried, with 4096 samples, ran overnight on desktop. My MacBook Pro w/16 threads and desktop with 12 threads and GPU are plenty powerful for this data set. Still, it’s useful to have the clustering option in the back pocket. In production, it may be more standard and maintainable to deploy with e.g. Terraform, Kubernetes than the Ray native YAML cluster config file. If you want to train big data at scale you need to really understand and streamline your pipeline.\n",
    "\n",
    "It continues to surprise me that ElasticNet, i.e. regularized linear regression, performs slightly better than boosting on this dataset. I heavily engineered features so that linear methods work well. Predictors were chosen using Lasso/ElasticNet and I used log and Box-Cox transforms to force predictors to follow assumptions of least-squares. But still, boosting is supposed to be the gold standard for tabular data.\n",
    "\n",
    "This may tend to validate one of the critiques of machine learning, that the most powerful machine learning methods don’t necessarily always converge all the way to the best solution. If you have a ground truth that is linear plus noise, a complex XGBoost or neural network algorithm should get arbitrarily close to the closed-form optimal solution, but will probably never match the optimal solution exactly. XGBoost regression is piecewise constant and the complex neural network is subject to the vagaries of stochastic gradient descent. I thought arbitrarily close meant almost indistinguishable. But clearly this is not always the case.\n",
    "\n",
    "ElasticNet with L1 + L2 regularization plus gradient descent and hyperparameter optimization is still machine learning. It’s simply a form of ML better matched to this problem. In the real world where data sets don’t match assumptions of OLS, gradient boosting generally performs extremely well. And even on this dataset, engineered for success with the linear models, SVR and KernelRidge performed better than ElasticNet (not shown) and ensembling ElasticNet with XGBoost, LightGBM, SVR, neural networks worked best of all.\n",
    "To paraphrase Casey Stengel, clever feature engineering will always outperform clever model algorithms and vice-versa². But improving your hyperparameters will always improve your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!DOCTYPE html><html><head><title>table1.md – Medium</title><meta name=\"description\" content=\"GitHub Gist: instantly share code, notes, and snippets.\"><meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"><meta name=\"twitter:widgets:csp\" content=\"on\"><meta name=\"robots\" content=\"noindex\"><base target=\"_blank\"><style>body {text-rendering: optimizeLegibility; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; font-family: \"ff-tisa-web-pro\", Georgia, Cambria, \"Times New Roman\", Times, serif; font-weight: 400; color: #333332; font-size: 18px; line-height: 1.4; margin: 0; background-color: white; overflow: hidden;}iframe {max-width: 100%;}</style></head><body><style>.gist .gist-file { margin-bottom: 0 !important; }.gist { text-rendering: auto; }</style><script src=\"https://gist.github.com/druce/e01308522cdfe0b9095e6e9957dd3ed6.js\" charset=\"utf-8\"></script><script>var height = -1; var delayMs = 200; if (document) {document.domain = document.domain;}function notifyResize(height) {height = height ? height : document.documentElement.offsetHeight; var resized = false; if (window.donkey && donkey.resize) {donkey.resize(height);var elements = document.getElementsByClassName(\"gist-data\"); for (var i = 0; i < elements.length; i++) {elements[i].style.overflow = \"visible\"}resized = true;}if (parent && parent._resizeIframe) {var obj = {iframe: window.frameElement, height: height}; parent._resizeIframe(obj); resized = true;}if (window.location && window.location.hash === \"#amp=1\" && window.parent && window.parent.postMessage) {window.parent.postMessage({sentinel: \"amp\", type: \"embed-size\", height: height}, \"*\");}if (window.webkit && window.webkit.messageHandlers && window.webkit.messageHandlers.resize) {window.webkit.messageHandlers.resize.postMessage(height); resized = true;}return resized;}function maybeResize() {try {if (document.documentElement.offsetHeight != height && notifyResize()) {height = document.documentElement.offsetHeight;}delayMs = Math.min(delayMs * 2, 1000000); setTimeout(maybeResize, delayMs);} catch(error) {console.log('maybeResize error: ', error)}}maybeResize();</script></body></html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6)Evaluate your result on both train and test set. Analyse if there is any underfitting or overfitting problem. Make your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1500/1*vhuuTIB0tl1Ka39itIIoHQ.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Basics\n",
    "In order to talk about underfitting vs overfitting, we need to start with the basics: what is a model? A model is simply a system for mapping inputs to outputs. For example, if we want to predict house prices, we could make a model that takes in the square footage of a house and outputs a price. A model represents a theory about a problem: there is some connection between the square footage and the price and we make a model to learn that relationship. Models are useful because we can use them to predict the values of outputs for new data points given the inputs.\n",
    "\n",
    "A model learns relationships between the inputs, called features, and outputs, called labels, from a training dataset. During training the model is given both the features and the labels and learns how to map the former to the latter. A trained model is evaluated on a testing set, where we only give it the features and it makes predictions. We compare the predictions with the known labels for the testing set to calculate accuracy. Models can take many shapes, from simple linear regressions to deep neural networks, but all supervised models are based on the fundamental idea of learning relationships between inputs and outputs from training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Data\n",
    "To make a model, we first need data that has an underlying relationship. For this example, we will create our own simple dataset with x-values (features) and y-values (labels). An important part of our data generation is adding random noise to the labels.\n",
    "\n",
    "In any real-world process, whether natural or man-made, the data does not exactly fit to a trend. There is always noise or other variables in the relationship we cannot measure. In the house price example, the trend between area and price is linear, but the prices do not lie exactly on a line because of other factors influencing house prices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1050/1*khzX1tmUj86vfpzE6Gkf0A.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data similarly has a trend (which we call the true function) and random noise to make it more realistic.\n",
    "After creating the data, we split it into random training and testing sets. \n",
    "The model will attempt to learn the relationship on the training data and be evaluated on the test data. \n",
    "In this case, 70% of the data is used for training and 30% for testing. The following graph shows the data we will explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/870/1*5w7HRScKcuO1WZoPQFhjvw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building\n",
    "\n",
    "Choosing a model can seem intimidating, but a good rule is to start simple and then build your way up. The simplest model is a linear regression, where the outputs are a linearly weighted combination of the inputs. In our model, we will use an extension of linear regression called polynomial regression to learn the relationship between x and y. \n",
    "Polynomial regression, where the inputs are raised to different powers, is still considered a form of “linear” regression even though the graph does not form a straight line (this confused me at first as well!)The general equation for a polynomial is below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/930/1*-28slRIHFanLBJkFpBfKZA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here y represents the label and x is the feature.\n",
    "The beta terms are the model parameters which will be learned during training, and the epsilon is the error present in any model. Once the model has learned the beta values, we can plug in any value for x and get a corresponding prediction for y. \n",
    "A polynomial is defined by its order, which is the highest power of x in the equation. \n",
    "A straight line is a polynomial of degree 1 while a parabola has 2 degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1050/1*pjIp920-MZdS_3fLVhf-Dw.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting vs. Underfitting\n",
    "\n",
    "The problem of Overfitting vs Underfitting finally appears when we talk about the polynomial degree. The degree represents how much flexibility is in the model, with a higher power allowing the model freedom to hit as many data points as possible. An underfit model will be less flexible and cannot account for the data. The best way to understand the issue is to take a look at models demonstrating both situations.\n",
    "\n",
    "First up is an underfit model with a 1 degree polynomial fit. In the image on the left, model function in orange is shown on top of the true function and the training observations.\n",
    "\n",
    "On the right, the model predictions for the testing data are shown compared to the true function and testing data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/750/1*kZfqaD6hl9iYGYXkMwV-JA.png)\n",
    "![](https://miro.medium.com/max/750/1*2RXJ2O-_c2ukaq5p-WQ9tQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
